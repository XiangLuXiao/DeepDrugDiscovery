#!/usr/bin/env python3
"""
Leave-one-out validation workflow for similarity matrices generated by the pipeline.

Copyright (c) 2025 Xianglu Xiao
Author: Xianglu Xiao
"""

import pandas as pd
import numpy as np
import os

def main():
    """Simple leave-one-out validation: check top score for each left-out compound."""
    
    # Load compound data
    try:
        reference_df = pd.read_csv('./data/processed_data/reference_compounds_data.csv')
        compound_df = pd.read_csv('./data/processed_data/candidate_compounds_library_data.csv')
        print(f"Loaded reference compounds: {len(reference_df)}")
        print(f"Loaded candidate compounds: {len(compound_df)}")
        
    except FileNotFoundError:
        print("Error: The dataset files were not found.")
        return
    
    # Load similarity matrix
    try:
        sim_mat = np.load('data/similarity_metrics/sim_mat.npy')
        print(f"Loaded similarity matrix shape: {sim_mat.shape}")
        
    except FileNotFoundError:
        print("Error: Similarity matrix file not found.")
        return
    
    print("\n=== Leave-One-Out Validation ===")
    
    results = []
    similarity_threshold = 0.65
    
    # Test each reference compound
    for left_out_idx in range(len(reference_df)):
        left_out_name = reference_df.loc[left_out_idx, 'Compound name in studies']
        print(f"Testing {left_out_idx + 1}/{len(reference_df)}: {left_out_name}")
        
        # Get training compounds (all except left-out)
        training_indices = [i for i in range(len(reference_df)) if i != left_out_idx]
        
        # Find candidates with high similarity to training compounds
        training_candidates = set()
        for train_idx in training_indices:
            # Find high-similarity candidates for this training compound
            similarities = sim_mat[train_idx]
            high_sim_mask = (similarities >= similarity_threshold) & (similarities < 1.0)
            candidates = np.where(high_sim_mask)[0]
            training_candidates.update(candidates)
        
        # Check left-out compound's best score AND rank against training candidates
        if len(training_candidates) > 0:
            left_out_similarities = sim_mat[left_out_idx]
            candidate_scores = [left_out_similarities[idx] for idx in training_candidates]
            top_score = max(candidate_scores)
            
            # Calculate rank: how would the best training candidate rank among ALL candidates?
            all_similarities = left_out_similarities
            sorted_indices = np.argsort(all_similarities)[::-1]  # Descending order
            
            # Find rank of best training candidate
            best_rank = len(all_similarities)  # Worst possible rank
            for rank, candidate_idx in enumerate(sorted_indices):
                if candidate_idx in training_candidates:
                    best_rank = rank + 1  # 1-indexed rank
                    break
                    
        else:
            top_score = 0.0
            best_rank = len(sim_mat[left_out_idx])  # No training candidates
        
        results.append({
            'compound_name': left_out_name,
            'top_score': top_score,
            'best_rank': best_rank,
            'total_candidates': len(sim_mat[left_out_idx]),
            'training_library_size': len(training_candidates)
        })
        
        print(f"  Top score: {top_score:.4f}, Best rank: {best_rank}/{len(sim_mat[left_out_idx])}")
    
    # Summary
    results_df = pd.DataFrame(results)
    top_scores = results_df['top_score']
    best_ranks = results_df['best_rank']
    
    print(f"\n=== Results Summary ===")
    print(f"SCORING PERFORMANCE:")
    print(f"  Average top score: {top_scores.mean():.4f} ± {top_scores.std():.4f}")
    print(f"  Min top score: {top_scores.min():.4f}")
    print(f"  Max top score: {top_scores.max():.4f}")
    
    print(f"\nRANKING PERFORMANCE:")
    print(f"  Average ranking position: {best_ranks.mean():.1f}")
    print(f"  Median ranking position: {best_ranks.median():.1f}")
    print(f"  Best ranking position: {best_ranks.min()}")
    print(f"  Worst ranking position: {best_ranks.max()}")
    
    # Calculate advanced ranking metrics
    total_candidates = results_df['total_candidates'].iloc[0]  # Should be same for all
    
    # Hit Rates at multiple K values
    hit_at_100 = (best_ranks <= 100).mean()
    hit_at_500 = (best_ranks <= 500).mean()
    hit_at_1000 = (best_ranks <= 1000).mean()
    
    # Mean Reciprocal Rank (MRR)
    reciprocal_ranks = 1.0 / best_ranks
    mrr = reciprocal_ranks.mean()
    
    # Enrichment Factor (EF) - improvement over random selection
    random_hit_rate = len(results_df) / total_candidates
    ef_100 = hit_at_100 / (100 * random_hit_rate) if random_hit_rate > 0 else 0
    ef_500 = hit_at_500 / (500 * random_hit_rate) if random_hit_rate > 0 else 0
    ef_1000 = hit_at_1000 / (1000 * random_hit_rate) if random_hit_rate > 0 else 0
    
    # Normalized Discounted Cumulative Gain (NDCG@500)
    dcg = sum(1/np.log2(min(rank, 500) + 1) for rank in best_ranks if rank <= 500)
    idcg = sum(1/np.log2(i + 2) for i in range(min(len(best_ranks), 500)))  # Perfect ranking
    ndcg_500 = dcg / idcg if idcg > 0 else 0
    
    print(f"\nHIT RATES:")
    print(f"  Hit@100:  {(best_ranks <= 100).sum()}/{len(results_df)} ({hit_at_100*100:.1f}%)")
    print(f"  Hit@500:  {(best_ranks <= 500).sum()}/{len(results_df)} ({hit_at_500*100:.1f}%)")
    print(f"  Hit@1000: {(best_ranks <= 1000).sum()}/{len(results_df)} ({hit_at_1000*100:.1f}%)")
    
    print(f"\nRANKING METRICS:")
    print(f"  Mean Reciprocal Rank (MRR): {mrr:.4f}")
    print(f"  NDCG@500: {ndcg_500:.4f}")
    
    print(f"\nENRICHMENT FACTORS:")
    print(f"  EF@100:  {ef_100:.1f}x better than random")
    print(f"  EF@500:  {ef_500:.1f}x better than random") 
    print(f"  EF@1000: {ef_1000:.1f}x better than random")
    
    # Success rate by score threshold
    high_score_compounds = results_df[results_df['top_score'] >= 0.75]
    if len(high_score_compounds) > 0:
        hit_rate_high_score = (high_score_compounds['best_rank'] <= 500).mean()
        print(f"\nHIGH-SCORING COMPOUNDS (score ≥ 0.75):")
        print(f"  Count: {len(high_score_compounds)}/{len(results_df)}")
        print(f"  Hit@500 for high-scoring: {hit_rate_high_score*100:.1f}%")
    
    print(f"\nSCORING SUCCESS:")
    print(f"  Score ≥ 0.65: {(top_scores >= 0.65).sum()}/{len(results_df)} ({(top_scores >= 0.65).mean()*100:.1f}%)")
    print(f"  Score ≥ 0.70: {(top_scores >= 0.70).sum()}/{len(results_df)} ({(top_scores >= 0.70).mean()*100:.1f}%)")
    
    # Show worst performers by rank
    worst = results_df.nlargest(5, 'best_rank')
    print(f"\nWorst ranking compounds:")
    for _, row in worst.iterrows():
        print(f"  {row['compound_name']}: score={row['top_score']:.4f}, rank={row['best_rank']}")
    
    # Show best performers
    best = results_df.nsmallest(5, 'best_rank')
    print(f"\nBest ranking compounds:")
    for _, row in best.iterrows():
        print(f"  {row['compound_name']}: score={row['top_score']:.4f}, rank={row['best_rank']}")
    
    # Save results
    os.makedirs('./data/validation_results', exist_ok=True)
    results_df.to_csv('./data/validation_results/simple_loo_validation.csv', index=False)
    print(f"\nResults saved to: ./data/validation_results/simple_loo_validation.csv")
    
    return results_df

if __name__ == "__main__":
    results = main()
